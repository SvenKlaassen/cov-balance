{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import doubleml as dml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# Custom Transformer to subsample labeled observations\n",
    "class SubsampleTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, subsample_fraction=0.5):\n",
    "        self.subsample_fraction = subsample_fraction\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to fit in this transformer\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if y is None:\n",
    "            return X  # Only for fit_transform we need X, y together\n",
    "        assert all(elements in (1, 0) for elements in y), 'Labels must be binary'\n",
    "\n",
    "        treated = y == 1\n",
    "        control = y == 0\n",
    "\n",
    "        control_subsample = control\n",
    "        p = [self.subsample_fraction, 1 - self.subsample_fraction]\n",
    "        random_subsample = np.random.choice([True, False], size=control.sum(), p=p)\n",
    "        control_subsample[control] = random_subsample\n",
    "\n",
    "        indices = np.where(treated | control_subsample)[0]\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "# Custom Classifier that rescales predictions\n",
    "class RescalePredictionsClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_classifier=None, rescale_factor=1.0):\n",
    "        self.base_classifier = base_classifier if base_classifier else LogisticRegression()\n",
    "        self.rescale_factor = rescale_factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.base_classifier.predict_proba(X)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        base_proba = self.base_classifier.predict_proba(X)\n",
    "        proba = self.rescale_factor * base_proba / (self.rescale_factor * base_proba + (1 - base_proba))\n",
    "        # proba = np.column_stack([1 - proba, proba])\n",
    "        return proba\n",
    "\n",
    "\n",
    "# Function to apply the transformer only during fit\n",
    "def fit_transform_only(transformer, X, y=None):\n",
    "    return transformer.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31668643, 0.68331357],\n",
       "       [0.99773922, 0.00226078],\n",
       "       [0.98726358, 0.01273642],\n",
       "       ...,\n",
       "       [0.98592248, 0.01407752],\n",
       "       [0.99484974, 0.00515026],\n",
       "       [0.98243718, 0.01756282]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "class SubsampleTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, subsample_fraction=0.5):\n",
    "        self.subsample_fraction = subsample_fraction\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to fit in this transformer\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if y is None:\n",
    "            return X  # Only for fit_transform we need X, y together\n",
    "        assert all(elements in (1, 0) for elements in y), 'Labels must be binary'\n",
    "\n",
    "        treated = y == 1\n",
    "        control = y == 0\n",
    "\n",
    "        control_subsample = control\n",
    "        p = [self.subsample_fraction, 1 - self.subsample_fraction]\n",
    "        random_subsample = np.random.choice([True, False], size=control.sum(), p=p)\n",
    "        control_subsample[control] = random_subsample\n",
    "\n",
    "        indices = np.where(treated | control_subsample)[0]\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X, y)\n",
    "\n",
    "class RescalePredictionsClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_classifier=None, rescale_factor=1.0):\n",
    "        self.base_classifier = base_classifier if base_classifier else LogisticRegression()\n",
    "        self.rescale_factor = rescale_factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.predict_proba(X)\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        base_proba = self.base_classifier.predict_proba(X)[:, 1]\n",
    "        proba = self.rescale_factor * base_proba / (self.rescale_factor * base_proba + (1 - base_proba))\n",
    "        all_proba = np.column_stack([1 - proba, proba])\n",
    "        return all_proba\n",
    "\n",
    "# Function to apply the transformer only during fit\n",
    "def fit_transform_only(X, y=None, transformer=None):\n",
    "    return transformer.fit_transform(X, y)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', FunctionTransformer(func=fit_transform_only, kw_args={'transformer': SubsampleTransformer(subsample_fraction=1.0)}, validate=False)),\n",
    "    ('classifier', RescalePredictionsClassifier(rescale_factor=1.0))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use predict on the full pipeline\n",
    "y_pred = pipeline.predict_proba(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of treated in train set: 0.0534\n",
      "percentage of treated in test set: 0.06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>D</th>\n",
       "      <th>Y</th>\n",
       "      <th>m_oracle</th>\n",
       "      <th>m_hat</th>\n",
       "      <th>m_calibrated</th>\n",
       "      <th>m_oracle_ate_weights</th>\n",
       "      <th>m_oracle_att_weights</th>\n",
       "      <th>m_hat_ate_weights</th>\n",
       "      <th>m_hat_att_weights</th>\n",
       "      <th>m_calibrated_ate_weights</th>\n",
       "      <th>m_calibrated_att_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.372271</td>\n",
       "      <td>0.595549</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.728639</td>\n",
       "      <td>0.135755</td>\n",
       "      <td>0.139917</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.157079</td>\n",
       "      <td>0.157079</td>\n",
       "      <td>1.162679</td>\n",
       "      <td>0.162679</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.462103</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.338655</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.023815</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>1.021678</td>\n",
       "      <td>0.021678</td>\n",
       "      <td>1.024396</td>\n",
       "      <td>0.024396</td>\n",
       "      <td>1.023923</td>\n",
       "      <td>0.023923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.960046</td>\n",
       "      <td>0.240038</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.427715</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>1.003482</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>1.003432</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>1.001832</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.607569</td>\n",
       "      <td>0.934605</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.387937</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>1.003518</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>1.004433</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>1.001832</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.565393</td>\n",
       "      <td>0.395288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.465630</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>1.003982</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>1.004035</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>1.001832</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  X3  D         Y  m_oracle     m_hat  m_calibrated  \\\n",
       "0  1.372271  0.595549   0  0  3.728639  0.135755  0.139917      0.142857   \n",
       "1  0.462103  0.755682   0  0  1.338655  0.021218  0.023815      0.023364   \n",
       "2 -0.960046  0.240038   1  0  2.427715  0.003470  0.003420      0.001828   \n",
       "3 -0.607569  0.934605   1  0  2.387937  0.003506  0.004413      0.001828   \n",
       "4 -0.565393  0.395288   0  0  1.465630  0.003966  0.004019      0.001828   \n",
       "\n",
       "   m_oracle_ate_weights  m_oracle_att_weights  m_hat_ate_weights  \\\n",
       "0              1.157079              0.157079           1.162679   \n",
       "1              1.021678              0.021678           1.024396   \n",
       "2              1.003482              0.003482           1.003432   \n",
       "3              1.003518              0.003518           1.004433   \n",
       "4              1.003982              0.003982           1.004035   \n",
       "\n",
       "   m_hat_att_weights  m_calibrated_ate_weights  m_calibrated_att_weights  \n",
       "0           0.162679                  1.166667                  0.166667  \n",
       "1           0.024396                  1.023923                  0.023923  \n",
       "2           0.003432                  1.001832                  0.001832  \n",
       "3           0.004433                  1.001832                  0.001832  \n",
       "4           0.004035                  1.001832                  0.001832  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from csv\n",
    "dgp_name = \"unbalanced\"\n",
    "\n",
    "df_train = pd.read_csv(f\"../dgps/data/{dgp_name}_train.csv\")\n",
    "df_test = pd.read_csv(f\"../dgps/data/{dgp_name}_test.csv\")\n",
    "\n",
    "print(f\"percentage of treated in train set: {df_train['D'].mean()}\")\n",
    "print(f\"percentage of treated in test set: {df_test['D'].mean()}\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fraction = df_train['D'].mean()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('transformer', FunctionTransformer(func=fit_transform_only, kw_args={'transformer': SubsampleTransformer(subsample_fraction=fraction)}, validate=False)),\n",
    "    ('classifier', RescalePredictionsClassifier(rescale_factor=fraction)),\n",
    "])\n",
    "\n",
    "\n",
    "X_train = df_train[['X1', 'X2', 'X3']].values\n",
    "y_train = df_train['D'].values\n",
    "\n",
    "X_test = df_test[['X1', 'X2', 'X3']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use predict on the full pipeline\n",
    "y_pred = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10331679, 0.00012098, 0.00068842, 0.00072548, 0.01032256,\n",
       "       0.00012469, 0.00084842, 0.00110119, 0.0022832 , 0.00071451])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>D</th>\n",
       "      <th>Y</th>\n",
       "      <th>m_oracle</th>\n",
       "      <th>m_hat</th>\n",
       "      <th>m_calibrated</th>\n",
       "      <th>m_oracle_ate_weights</th>\n",
       "      <th>m_oracle_att_weights</th>\n",
       "      <th>m_hat_ate_weights</th>\n",
       "      <th>m_hat_att_weights</th>\n",
       "      <th>m_calibrated_ate_weights</th>\n",
       "      <th>m_calibrated_att_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.637882</td>\n",
       "      <td>0.457121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.535259</td>\n",
       "      <td>0.693948</td>\n",
       "      <td>0.683314</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>1.441030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.463457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.037066</td>\n",
       "      <td>0.655834</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056473</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>1.001970</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>1.002266</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>1.003229</td>\n",
       "      <td>0.003229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.349317</td>\n",
       "      <td>0.052052</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.743051</td>\n",
       "      <td>0.014054</td>\n",
       "      <td>0.012736</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>1.014254</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>1.012901</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>1.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.243824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.590433</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.013413</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>1.014399</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>1.013596</td>\n",
       "      <td>0.013596</td>\n",
       "      <td>1.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.074333</td>\n",
       "      <td>0.197206</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.203856</td>\n",
       "      <td>0.175297</td>\n",
       "      <td>0.163406</td>\n",
       "      <td>0.198473</td>\n",
       "      <td>1.212558</td>\n",
       "      <td>0.212558</td>\n",
       "      <td>1.195323</td>\n",
       "      <td>0.195323</td>\n",
       "      <td>1.247619</td>\n",
       "      <td>0.247619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.925890</td>\n",
       "      <td>0.135362</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.463885</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>1.002511</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>1.002335</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>1.003229</td>\n",
       "      <td>0.003229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.235615</td>\n",
       "      <td>0.725759</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.355423</td>\n",
       "      <td>0.014001</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>1.014200</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.015902</td>\n",
       "      <td>0.015902</td>\n",
       "      <td>1.021277</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.149456</td>\n",
       "      <td>0.044816</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.330547</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>1.023614</td>\n",
       "      <td>0.023614</td>\n",
       "      <td>1.020644</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>1.021277</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.683212</td>\n",
       "      <td>0.554894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.627412</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>0.048433</td>\n",
       "      <td>1.041235</td>\n",
       "      <td>0.041235</td>\n",
       "      <td>1.042854</td>\n",
       "      <td>0.042854</td>\n",
       "      <td>1.050898</td>\n",
       "      <td>0.050898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.055548</td>\n",
       "      <td>0.909041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.437770</td>\n",
       "      <td>0.010770</td>\n",
       "      <td>0.013213</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>1.010888</td>\n",
       "      <td>0.010888</td>\n",
       "      <td>1.013390</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>1.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  X3  D         Y  m_oracle     m_hat  m_calibrated  \\\n",
       "0  2.637882  0.457121   0  1  7.535259  0.693948  0.683314      0.769231   \n",
       "1 -1.037066  0.655834   1  0  0.056473  0.001966  0.002261      0.003219   \n",
       "2 -0.349317  0.052052   1  0  2.743051  0.014054  0.012736      0.009804   \n",
       "3  0.001619  0.243824   0  0  2.590433  0.014195  0.013413      0.009804   \n",
       "4  1.074333  0.197206   1  0  6.203856  0.175297  0.163406      0.198473   \n",
       "5 -0.925890  0.135362   0  0  1.463885  0.002505  0.002330      0.003219   \n",
       "6  0.235615  0.725759   0  0  1.355423  0.014001  0.015653      0.020833   \n",
       "7  0.149456  0.044816   0  0  3.330547  0.023069  0.020227      0.020833   \n",
       "8  0.683212  0.554894   0  0  3.627412  0.039602  0.041093      0.048433   \n",
       "9 -0.055548  0.909041   1  0  2.437770  0.010770  0.013213      0.009804   \n",
       "\n",
       "   m_oracle_ate_weights  m_oracle_att_weights  m_hat_ate_weights  \\\n",
       "0              1.441030              1.000000           1.463457   \n",
       "1              1.001970              0.001970           1.002266   \n",
       "2              1.014254              0.014254           1.012901   \n",
       "3              1.014399              0.014399           1.013596   \n",
       "4              1.212558              0.212558           1.195323   \n",
       "5              1.002511              0.002511           1.002335   \n",
       "6              1.014200              0.014200           1.015902   \n",
       "7              1.023614              0.023614           1.020644   \n",
       "8              1.041235              0.041235           1.042854   \n",
       "9              1.010888              0.010888           1.013390   \n",
       "\n",
       "   m_hat_att_weights  m_calibrated_ate_weights  m_calibrated_att_weights  \n",
       "0           1.000000                  1.300000                  1.000000  \n",
       "1           0.002266                  1.003229                  0.003229  \n",
       "2           0.012901                  1.009901                  0.009901  \n",
       "3           0.013596                  1.009901                  0.009901  \n",
       "4           0.195323                  1.247619                  0.247619  \n",
       "5           0.002335                  1.003229                  0.003229  \n",
       "6           0.015902                  1.021277                  0.021277  \n",
       "7           0.020644                  1.021277                  0.021277  \n",
       "8           0.042854                  1.050898                  0.050898  \n",
       "9           0.013390                  1.009901                  0.009901  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_data = dml.DoubleMLData(df_train, 'Y', 'D', ['X1', 'X2', 'X3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== DoubleMLIRM Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: Y\n",
      "Treatment variable(s): ['D']\n",
      "Covariates: ['X1', 'X2', 'X3']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 5000\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: ATE\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "Learner ml_g: LinearRegression()\n",
      "Learner ml_m: LogisticRegression()\n",
      "Out-of-sample Performance:\n",
      "Regression:\n",
      "Learner ml_g0 RMSE: [[0.50367412]]\n",
      "Learner ml_g1 RMSE: [[0.50319678]]\n",
      "Classification:\n",
      "Learner ml_m Log Loss: [[0.14389963]]\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 5\n",
      "No. repeated sample splits: 1\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      "       coef   std err         t  P>|t|     2.5 %    97.5 %\n",
      "D  1.972574  0.040441  48.77695    0.0  1.893311  2.051836\n"
     ]
    }
   ],
   "source": [
    "dml_model = dml.DoubleMLIRM(dml_data,\n",
    "                            ml_g=LinearRegression(),\n",
    "                            ml_m=LogisticRegression(),\n",
    "                            n_folds=5)\n",
    "\n",
    "dml_model.fit()\n",
    "print(dml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RescalePredictionsClassifier' object has no attribute 'classes_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m dml_model \u001b[38;5;241m=\u001b[39m dml\u001b[38;5;241m.\u001b[39mDoubleMLIRM(dml_data,\n\u001b[0;32m      2\u001b[0m                             ml_g\u001b[38;5;241m=\u001b[39mLinearRegression(),\n\u001b[0;32m      3\u001b[0m                             ml_m\u001b[38;5;241m=\u001b[39mPipeline([\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m                             ]),\n\u001b[0;32m      7\u001b[0m                             n_folds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mdml_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(dml_model)\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\doubleml\\double_ml.py:503\u001b[0m, in \u001b[0;36mDoubleML.fit\u001b[1;34m(self, n_jobs_cv, store_predictions, external_predictions, store_models)\u001b[0m\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dml_data\u001b[38;5;241m.\u001b[39mset_x_d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dml_data\u001b[38;5;241m.\u001b[39md_cols[i_d])\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# predictions have to be stored in loop for sensitivity analysis\u001b[39;00m\n\u001b[1;32m--> 503\u001b[0m nuisance_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_nuisance_and_score_elements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexternal_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_score_and_estimate_se()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;66;03m# sensitivity elements can depend on the estimated parameter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\doubleml\\double_ml.py:966\u001b[0m, in \u001b[0;36mDoubleML._fit_nuisance_and_score_elements\u001b[1;34m(self, n_jobs_cv, store_predictions, external_predictions, store_models)\u001b[0m\n\u001b[0;32m    960\u001b[0m ext_prediction_dict \u001b[38;5;241m=\u001b[39m _set_external_predictions(external_predictions,\n\u001b[0;32m    961\u001b[0m                                                 learners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_names,\n\u001b[0;32m    962\u001b[0m                                                 treatment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dml_data\u001b[38;5;241m.\u001b[39md_cols[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_treat],\n\u001b[0;32m    963\u001b[0m                                                 i_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_rep)\n\u001b[0;32m    965\u001b[0m \u001b[38;5;66;03m# ml estimation of nuisance models and computation of score elements\u001b[39;00m\n\u001b[1;32m--> 966\u001b[0m score_elements, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nuisance_est\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__smpls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mexternal_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext_prediction_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mreturn_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_score_elements(score_elements, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_rep, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_treat)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# calculate nuisance losses and store predictions and targets of the nuisance models\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\doubleml\\irm\\irm.py:303\u001b[0m, in \u001b[0;36mDoubleMLIRM._nuisance_est\u001b[1;34m(self, smpls, n_jobs_cv, external_predictions, return_models)\u001b[0m\n\u001b[0;32m    299\u001b[0m     m_hat \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m: external_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    300\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     m_hat \u001b[38;5;241m=\u001b[39m \u001b[43m_dml_cv_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmpls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmpls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs_cv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mest_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_method\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mreturn_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     _check_finite_predictions(m_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m, smpls)\n\u001b[0;32m    307\u001b[0m     _check_is_propensity(m_hat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_m\u001b[39m\u001b[38;5;124m'\u001b[39m, smpls, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\doubleml\\utils\\_estimation.py:63\u001b[0m, in \u001b[0;36m_dml_cv_predict\u001b[1;34m(estimator, x, y, smpls, n_jobs, est_params, method, return_train_preds, return_models)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m manual_cv_predict:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# if there are no parameters set we redirect to the standard method\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m         preds \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmpls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(est_params, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1282\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m-> 1282\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_predict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1295\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1296\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1391\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, fit_params, method)\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m         \u001b[38;5;66;03m# A 2D y array should be a binary label indicator matrix\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m         n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(y)) \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1390\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m _enforce_prediction_order(\n\u001b[1;32m-> 1391\u001b[0m             \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m, predictions, n_classes, method\n\u001b[0;32m   1392\u001b[0m         )\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\svenk\\.conda\\envs\\calibration\\Lib\\site-packages\\sklearn\\pipeline.py:1019\u001b[0m, in \u001b[0;36mPipeline.classes_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclasses_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The classes labels. Only exist if the last step is a classifier.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RescalePredictionsClassifier' object has no attribute 'classes_'"
     ]
    }
   ],
   "source": [
    "dml_model = dml.DoubleMLIRM(dml_data,\n",
    "                            ml_g=LinearRegression(),\n",
    "                            ml_m=Pipeline([\n",
    "                                ('transformer', FunctionTransformer(func=fit_transform_only, kw_args={'transformer': SubsampleTransformer(subsample_fraction=fraction)}, validate=False)),\n",
    "                                ('classifier', RescalePredictionsClassifier(rescale_factor=fraction)),\n",
    "                            ]),\n",
    "                            n_folds=5)\n",
    "\n",
    "dml_model.fit()\n",
    "print(dml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
